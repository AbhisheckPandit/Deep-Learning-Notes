{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification 1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOi4Ma6KXskH6wlGpPBPWxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalchaubey/Deep-Learning-Notes/blob/master/FB%20DevC%20Sweden%20April%20Image%20Classification%20in%202020/Image_Classification_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut1uDoXJzCTu",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification using PyTorch in 2020  \n",
        "\n",
        "In this notebook we will be utilizing some of the latest advancements in the  \n",
        "[PyTorch Ecosystem](https://pytorch.org/ecosystem/) to build a simple image classifier using CNNs.   \n",
        "\n",
        "Along the way, we will learn some PyTorch and CNN (Convolution Neural  \n",
        "Networks) basics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFhLAYud0CQK",
        "colab_type": "text"
      },
      "source": [
        "### 1. Get the Dataset Onboard\n",
        "\n",
        "In any Machine Learning/Data Science problem, the first step is always to get  \n",
        "the dataset.  \n",
        "\n",
        "In our case, to get things started, we will initially use the simple [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database).  \n",
        "MNIST is largely considered the _'Hello World!'_ of AI/ML. The dataset was  \n",
        "created way back in the late 90s. The [official description](http://yann.lecun.com/exdb/mnist/) states,  \n",
        "\n",
        "_\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image._  \n",
        "\n",
        "_It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\"_  \n",
        "\n",
        "<br/>You might be wondering, how to get this dataset in our Colab Workspace?  \n",
        "PyTorch comes with a _datasets_ module called, [Torchvision.Datasets](https://pytorch.org/docs/stable/torchvision/datasets.html).  \n",
        "Torchvision.Datasets module contains a number of publically available datasets  \n",
        "including the one we are looking for, MNIST. You are encouraged to explore the  \n",
        "Torchvision.Datasets documentation page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tia9JXE46rJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets import some libraries \n",
        "from torchvision import datasets # Datasets module \n",
        "import torchvision.transforms as transforms # Image Transforms \n",
        "from torch.utils.data.sampler import SubsetRandomSampler # Sampler "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVP4itgp7jC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Data Science Regulars\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8vdeaZ7sj_",
        "colab_type": "text"
      },
      "source": [
        "Checking out the torchvision.datasets module documentation, we find  \n",
        "![Torchvision.Dataset](https://drive.google.com/uc?id=1Zsgc5_PnO9BQQ5wqssf67A5Ge-qIXtLh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UapF3_qS67Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # do not modify \n",
        "# how many samples per batch to load\n",
        "batch_size = 20 # ie 20 images per batch \n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2 # ie Train Set divided into two parts \n",
        "                 # 80% Train 20% Validation \n",
        "\n",
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnZrOAcMZRTp",
        "colab_type": "text"
      },
      "source": [
        "### Train Validation Test Split \n",
        "\n",
        "Once the download is complete (usually instantaneous), you should be able to  \n",
        "see the MNIST dataset downloaded inside the _'data'_ folder on the left hand  \n",
        "side. (Click on the _Files_ icon on the left sidebar)  \n",
        "\n",
        "We have both the training and the test sets. Now we need to bifurcate the   \n",
        "training set in two parts,  \n",
        "1. Training Set (80% images)\n",
        "2. Validation Set (20% images)  \n",
        "\n",
        "The algorithm we use to do this is quite simple,  \n",
        "1. Create a list of indices of the training data \n",
        "2. Randomly Shuffle those indices \n",
        "3. Slice the indices in 80-20 split \n",
        "\n",
        "[Why create a _Validation Set_ at all?](https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrsPBIfRYwNc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f5d40839-143f-45b4-daa6-eb2621639c87"
      },
      "source": [
        "# obtain training indices that will be used for validation\n",
        "\n",
        "# 1. Create a list of indices of the training data  \n",
        "num_train = len(train_data)\n",
        "print('num_train = len(train_data) ==> ', num_train)\n",
        "indices = list(range(num_train))\n",
        "print('len(indices) ==>', len(indices))\n",
        "# print(indices)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_train = len(train_data) ==>  60000\n",
            "len(indices) ==> 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDu_dd35TZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Randomly Shuffle those indices\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKqrNSg-5VMq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "437a7ae5-1015-46f4-87bb-1d1d98e66476"
      },
      "source": [
        "# 3. Slice the indices in 80-20 split\n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "print('len(train_idx) ==> ', len(train_idx))\n",
        "print('len(valid_idx) ==> ', len(valid_idx))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_idx) ==>  48000\n",
            "len(valid_idx) ==>  12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14RHH0Jt8_k_",
        "colab_type": "text"
      },
      "source": [
        "Please Note that so far we have just been fiddling around with the _'indices'_,  \n",
        "not the actual images as such. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Zzb55d8k7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}