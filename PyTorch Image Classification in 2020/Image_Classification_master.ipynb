{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Classification_master.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPKGpnZKyFYsoUs95Iph6fr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalchaubey/Deep-Learning-Notes/blob/master/PyTorch%20Image%20Classification%20in%202020/Image_Classification_master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut1uDoXJzCTu",
        "colab_type": "text"
      },
      "source": [
        "# Image Classification using PyTorch in 2020  \n",
        "\n",
        "In this notebook we will be utilizing some of the latest advancements in the  \n",
        "[PyTorch Ecosystem](https://pytorch.org/ecosystem/) to build a simple image classifier using CNNs.   \n",
        "\n",
        "Along the way, we will learn some PyTorch and CNN (Convolution Neural  \n",
        "Networks) basics. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFhLAYud0CQK",
        "colab_type": "text"
      },
      "source": [
        "### 1. Get the Dataset Onboard\n",
        "\n",
        "In any Machine Learning/Data Science problem, the first step is always to get  \n",
        "the dataset.  \n",
        "\n",
        "In our case, to get things started, we will initially use the simple [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database).  \n",
        "MNIST is largely considered the _'Hello World!'_ of AI/ML. The dataset was  \n",
        "created way back in the late 90s. The [official description](http://yann.lecun.com/exdb/mnist/) states,  \n",
        "\n",
        "_\"The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image._  \n",
        "\n",
        "_It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting.\"_  \n",
        "\n",
        "<br/>You might be wondering, how to get this dataset in our Colab Workspace?  \n",
        "PyTorch comes with a _datasets_ module called, [Torchvision.Datasets](https://pytorch.org/docs/stable/torchvision/datasets.html).  \n",
        "Torchvision.Datasets module contains a number of publically available datasets  \n",
        "including the one we are looking for, MNIST. You are encouraged to explore the  \n",
        "Torchvision.Datasets documentation page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tia9JXE46rJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets import some libraries \n",
        "import torch # PyTorch \n",
        "from torchvision import datasets # Datasets module \n",
        "import torchvision.transforms as transforms # Image Transforms \n",
        "from torch.utils.data.sampler import SubsetRandomSampler # Sampler "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVP4itgp7jC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Data Science Regulars\n",
        "import numpy as np \n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6h8vdeaZ7sj_",
        "colab_type": "text"
      },
      "source": [
        "Checking out the torchvision.datasets module documentation, we find  \n",
        "![Torchvision.Dataset](https://drive.google.com/uc?id=1Zsgc5_PnO9BQQ5wqssf67A5Ge-qIXtLh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UapF3_qS67Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert data to torch.FloatTensor\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.MNIST(root='data', train=True,\n",
        "                                   download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False,\n",
        "                                  download=True, transform=transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnZrOAcMZRTp",
        "colab_type": "text"
      },
      "source": [
        "### 2. Train Validation Test Split \n",
        "\n",
        "Once the download is complete (usually instantaneous), you should be able to  \n",
        "see the MNIST dataset downloaded inside the _'data'_ folder on the left hand  \n",
        "side. (Click on the _Files_ icon on the left sidebar)  \n",
        "\n",
        "We have both the training and the test sets. Now we need to bifurcate the   \n",
        "training set in two parts,  \n",
        "1. Training Set (80% images)\n",
        "2. Validation Set (20% images)  \n",
        "\n",
        "The algorithm we use to do this is quite simple,  \n",
        "1. Create a list of indices of the training data \n",
        "2. Randomly Shuffle those indices \n",
        "3. Slice the indices in 80-20 split \n",
        "\n",
        "[Why create a _Validation Set_ at all?](https://datascience.stackexchange.com/questions/18339/why-use-both-validation-set-and-test-set) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrsPBIfRYwNc",
        "colab_type": "code",
        "outputId": "02bd9b99-09b8-44ec-a395-d302a9db2052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# obtain training indices that will be used for validation\n",
        "\n",
        "# 1. Create a list of indices of the training data  \n",
        "num_train = len(train_data)\n",
        "print('num_train = len(train_data) ==> ', num_train)\n",
        "indices = list(range(num_train))\n",
        "print('len(indices) ==>', len(indices))\n",
        "# print(indices)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num_train = len(train_data) ==>  60000\n",
            "len(indices) ==> 60000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjDu_dd35TZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. Randomly Shuffle those indices\n",
        "np.random.shuffle(indices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKqrNSg-5VMq",
        "colab_type": "code",
        "outputId": "b8b3913f-35fc-44b1-8cf4-58ee02c71070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# 3. Slice the indices in 80-20 split\n",
        "# percentage of training set to use as validation\n",
        "valid_size = 0.2 # ie Train Set divided into two parts \n",
        "                 # 80% Train 20% Validation \n",
        "split = int(np.floor(valid_size * num_train))\n",
        "train_idx, valid_idx = indices[split:], indices[:split]\n",
        "\n",
        "print('len(train_idx) ==> ', len(train_idx))\n",
        "print('len(valid_idx) ==> ', len(valid_idx))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(train_idx) ==>  48000\n",
            "len(valid_idx) ==>  12000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14RHH0Jt8_k_",
        "colab_type": "text"
      },
      "source": [
        "Please Note that so far we have just been fiddling around with the _'indices'_,  \n",
        "not the actual images as such.....but Why?  \n",
        "Answer below.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlTpuhxVAfCK",
        "colab_type": "text"
      },
      "source": [
        "### 3. Prepare the Dataloaders \n",
        "\n",
        "By now, we have downloaded the dataset, and created a train/valid/test split.  \n",
        "Q: How do we _'push'_ this data into a PyTorch model?  \n",
        "A: PyTorch has a mechanism to _'ingest'_ data from a dataset through a module  \n",
        "known as `DataLoader`.  \n",
        "\n",
        "A great analogy,  \n",
        "![DataLoader](https://drive.google.com/uc?id=1U4IG-5lbFGQQS4xwQPU2QiYdR1hFGBZ5)\n",
        "\n",
        "[Great tutorial on DataLoaders.](https://www.journaldev.com/36576/pytorch-dataloader)  \n",
        "[Ultimate tutorial on DataLoaders.](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel) \n",
        "\n",
        "Time to prepare the _DataLoaders_ now!  \n",
        "\n",
        "![DataLoader Documentation](https://drive.google.com/uc?id=1YFbWIGwNlL5Kp4Zvt52Ck0_Wk4MNfxS9)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_Zzb55d8k7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define samplers for obtaining training and validation batches\n",
        "# remember train_idx and valid_idx were the indices that we shuffled above\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "valid_sampler = SubsetRandomSampler(valid_idx)\n",
        "\n",
        "# prepare dataloaders\n",
        "# number of subprocesses to use for data loading\n",
        "num_workers = 0 # do not modify \n",
        "# how many samples per batch to load\n",
        "batch_size = 20 # ie 20 images per batch \n",
        "\n",
        "# Training Set \n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data, \\\n",
        "                                           batch_size=batch_size, \\\n",
        "                                           sampler=train_sampler, \\\n",
        "                                           num_workers=num_workers)\n",
        "# Validation Set \n",
        "valid_loader = torch.utils.data.DataLoader(dataset=train_data, \\\n",
        "                                           batch_size=batch_size, \\\n",
        "                                           sampler=valid_sampler, \\\n",
        "                                           num_workers=num_workers)\n",
        "# Test Set \n",
        "# Notice we have not used a 'sampler' here as it was not required \n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data, \\\n",
        "                                          batch_size=batch_size, \\\n",
        "                                          num_workers=num_workers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBy_7mS_4hdc",
        "colab_type": "text"
      },
      "source": [
        "We got the dataloaders working, but how do we know that they are working indeed?  \n",
        "Visualizing the data from the dataloaders would be a good check! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrRjUiQG5uOu",
        "colab_type": "code",
        "outputId": "0a0955a3-dbc1-4cfe-e545-eb3d686670ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "# Visualize a whole batch of data from the dataloaders \n",
        "\n",
        "dataiter = iter(train_loader) # Get the whole batch \n",
        "images, labels = dataiter.next() # Extract the images and their labels \n",
        "print(len(images), len(labels)) # Should be equal to the batch size, 20\n",
        "print('Correct Labels: ', labels)\n",
        "images = images.numpy() # Convert the images to numpy array for matplotlib\n",
        "print('Shape of our images tensor =', images.shape)\n",
        "print('Batch Size =', images.shape[0], 'Image Height/Width =', \\\n",
        "                                                        images.shape[2])\n",
        "\n",
        "print()\n",
        "print('Squeezing the images tensor =', np.squeeze(images).shape)\n",
        "print('Un-squeezing the images tensor (axis=1) =', \\\n",
        "                                        np.expand_dims(images, axis=1).shape)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20 20\n",
            "Correct Labels:  tensor([6, 5, 6, 3, 6, 0, 8, 4, 3, 7, 9, 8, 9, 0, 4, 8, 7, 6, 1, 1])\n",
            "Shape of our images tensor = (20, 1, 28, 28)\n",
            "Batch Size = 20 Image Height/Width = 28\n",
            "\n",
            "Squeezing the images tensor = (20, 28, 28)\n",
            "Un-squeezing the images tensor (axis=1) = (20, 1, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y58P0-etIck4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "# Plots are plotted inside the notebooks, 'inline'\n",
        "%matplotlib inline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yL8sSrheGg9-",
        "colab_type": "text"
      },
      "source": [
        "With matplotlib, always remember that _figures contain axes which in turn   \n",
        "contain the plots_.  \n",
        "![Real Python](https://drive.google.com/uc?id=1KdlAGoCK8Lj9pFkrZf52oqOJK3sH3JuH)  \n",
        "\n",
        "[Great tutorial on Matplotlib.](https://realpython.com/python-matplotlib-guide/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC5KTKSv6Y8T",
        "colab_type": "code",
        "outputId": "5660b42d-4235-4eb4-8c53-5783a306451b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# Plot the whole batch \n",
        "fig = plt.figure(figsize=(25, 4))\n",
        "# Loop over all the images in the batch(20)\n",
        "for idx in np.arange(20):\n",
        "    # Add a subplot for the image \n",
        "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
        "    # Populate the subplot with the image \n",
        "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
        "    # print out the correct label for each image\n",
        "    # .item() gets the value contained in a Tensor\n",
        "    ax.set_title(str(labels[idx].item()))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABXEAAAD7CAYAAAAsAtcsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 1800x288 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls_AFh0FzMkG",
        "colab_type": "code",
        "outputId": "377fe28c-cdbd-4dc9-b176-c064a49916e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        }
      },
      "source": [
        "# A neat little numpy trick\n",
        "# Just for fun! \n",
        "\n",
        "dataiter = iter(train_loader)  \n",
        "images, labels = dataiter.next() \n",
        "images = images.numpy() \n",
        "np.set_printoptions(precision=2, threshold=None, edgeitems=None, \\\n",
        "                    linewidth=180, suppress=None)\n",
        "print('Label', labels[0])\n",
        "print(images[0])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label tensor(0)\n",
            "[[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.32 0.84 0.99 0.32 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.16 0.95 0.99 0.99 0.8  0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.84 0.99 1.   0.99 1.   0.44 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.08 0.72 0.99 0.99 0.99 0.99 0.99 0.75 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.08 0.68 0.99 1.   0.99 0.72 0.48 1.   0.99 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.56 0.99 0.99 0.99 0.35 0.   0.4  0.99 0.99 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.2  0.91 1.   0.91 0.32 0.   0.   0.   1.   0.99 0.08 0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.6  0.99 0.91 0.2  0.   0.   0.   0.   0.84 0.99 0.24 0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.08 1.   0.99 0.4  0.   0.   0.   0.   0.   0.6  0.99 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.56 0.99 0.83 0.08 0.   0.   0.   0.   0.   0.6  0.99 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.04 0.84 1.   0.51 0.   0.   0.   0.   0.   0.08 1.   0.99 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.2  0.99 0.99 0.2  0.   0.   0.   0.   0.   0.4  0.99 0.99 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.84 0.99 0.56 0.   0.   0.   0.   0.   0.   0.8  1.   0.51 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.99 0.99 0.4  0.   0.   0.   0.   0.   0.08 0.87 0.84 0.04 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.24 1.   0.99 0.4  0.   0.   0.   0.   0.   0.84 0.99 0.32 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.08 0.99 0.99 0.4  0.   0.   0.   0.   0.64 0.99 0.83 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.99 0.48 0.   0.   0.16 0.68 0.99 0.96 0.32 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.51 0.99 0.95 0.64 0.8  0.95 0.99 0.99 0.48 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.08 0.87 1.   0.99 1.   0.99 0.96 0.32 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.16 0.59 0.91 0.91 0.44 0.16 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
            "  [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  ]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2APl70a5LH66",
        "colab_type": "text"
      },
      "source": [
        "Our dataloaders seem to be working fine and out data looks great!  \n",
        "<br/> \n",
        "Time to build our CNN based image classification model in PyTorch.....in 2020! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGFnKhH5Lh9t",
        "colab_type": "text"
      },
      "source": [
        "### 4. Build a PyTorch CNN Model\n",
        "But first, we need to know what are the components of a typical CNN based  \n",
        "image classification architecture. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjUD_2bKKteP",
        "colab_type": "text"
      },
      "source": [
        "| INSERT CNN INTRODUCTION | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGqPxOlPLGUT",
        "colab_type": "text"
      },
      "source": [
        "| INSERT PYTORCH INTRODUCTION | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYblCGur448H",
        "colab_type": "text"
      },
      "source": [
        "Time to define our model!  \n",
        "But before we start off with the model definition, let's have a look at what  \n",
        "the PyTorch Documentation says about the Convolutional NNs.  \n",
        "![Conv2d Layer](https://drive.google.com/uc?id=1odHZIXURYjogjUcyCQ56RYfQPqgzO7DX \"Conv2D\")  \n",
        "\n",
        "We also need to check out about the MaxPool, Dropout and Linear Layers.  \n",
        "\n",
        "![MaxPool 2D](https://drive.google.com/uc?id=1r1EeLHrV5oAG4OUyRSm0lw2OcZyJQ7ne \"MaxPool2D Layer\")  \n",
        "\n",
        "![Dropout Layer](https://drive.google.com/uc?id=1kYgb4wDrGEBEF5WB169N7Q5hTrwDDO1d \"Dropout Layer\")  \n",
        "\n",
        "![Linear Layer](https://drive.google.com/uc?id=1rrIEqPtun_8Td1js76B2hv22Xm5tjY6m \"Linear Layer\")  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0zv4wi7BNRL",
        "colab_type": "text"
      },
      "source": [
        "Did you guys notice a weird anomaly in the Conv2d and Linear layers?  \n",
        "```\n",
        "torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
        "```  \n",
        "```\n",
        "torch.nn.Linear(in_features, out_features, bias=True)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrhOYd8JTQ-B",
        "colab_type": "text"
      },
      "source": [
        "Conv2d layer expects `in_channels` while the Linear layer expects `in_features`.  \n",
        "\n",
        "Bottom line is, that PyTorch expects different things from a tensor dimension.  \n",
        "Specifically,  \n",
        "```\n",
        "\"\"\"Example tensor size outputs, how PyTorch reads them, and where you encounter them in the wild. \n",
        "Note: the values below are only examples. Focus on the rank of the tensor (how many dimensions it has).\"\"\"\n",
        ">>> torch.Size([32])\n",
        "    # 1d: [batch_size] \n",
        "    # use for target labels or predictions.\n",
        ">>> torch.Size([12, 256])\n",
        "    # 2d: [batch_size, num_features (aka: C * H * W)]\n",
        "    # use for as nn.Linear() input.\n",
        ">>> torch.Size([10, 1, 2048])\n",
        "    # 3d: [batch_size, channels, num_features (aka: H * W)]\n",
        "    # when used as nn.Conv1d() input.\n",
        "    # (but [seq_len, batch_size, num_features]\n",
        "    # if feeding an RNN).\n",
        ">>> torch.Size([16, 3, 28, 28])\n",
        "    # 4d: [batch_size, channels, height, width]\n",
        "    # use for as nn.Conv2d() input.\n",
        ">>>  torch.Size([32, 1, 5, 15, 15])\n",
        "    # 5D: [batch_size, channels, depth, height, width]\n",
        "    # use for as nn.Conv3d() input.\n",
        "```    \n",
        "\n",
        "A neat method to make your tensors ready for the linear layer,  \n",
        "```\n",
        "Use view() to change your tensor’s dimensions.\n",
        "\n",
        "image = image.view(batch_size, -1)\n",
        "\n",
        "You supply your batch_size as the first number, and then “-1” basically tells Pytorch, “you figure out this other number for me… please.” \n",
        "Your tensor will now feed properly into any linear layer.\n",
        "```\n",
        "\n",
        "[Incredible Tutorial on PyTorch Layer Dimensions.](https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouQTXZetn53I",
        "colab_type": "text"
      },
      "source": [
        "Before we start,  \n",
        "1. What is the shape (dimensions) of our images?  \n",
        "2. What is the shape of our batches? \n",
        "3. How many _'channels'_ are there in our images? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-McCK6u3WnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn # nn module contains all the layers \n",
        "import torch.nn.functional as F # same as nn, but a little different "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtUDongrrJlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our CNN based neural architecture \n",
        "# Let's build a simple one with only Convolutional, Linear \n",
        "# and dropout layers\n",
        "class MNISTModel1(nn.Module):\n",
        "    # Here we define the neural architecture \n",
        "    def __init__(self):\n",
        "        super(MNISTModel1, self).__init__() # Initialize the nn module \n",
        "        \n",
        "        # Convolutional Layers\n",
        "        # What shape/dimensions the first layer is going to see? \n",
        "        # Do we need to have some padding for a kernel_size = 3?  \n",
        "        # Input Features = 1 x 28 x 28\n",
        "        # Output Features = ???\n",
        "        # Shape of a Convolutional Layer = (W - K + 2P)\n",
        "        #                                  ------------ + 1\n",
        "        #                                       S\n",
        "        # where, \n",
        "        #       W = Width/Height of previous layer = 28\n",
        "        #       K = Filter Size = 3\n",
        "        #       P = Padding = 0\n",
        "        #       S = Stride = 1(default)\n",
        "        # Therefore, \n",
        "        #           if padding = 0\n",
        "        #           Output Shape = ((28 - 3 + 2*0)/1)+1 = 26 \n",
        "        # We want the dimensions to stay the same so that there is no \n",
        "        # loss of information when performing the convolution. \n",
        "        # Hence, \n",
        "        #       if padding = 1\n",
        "        #       Output Shape = ((28 - 3 + 2*1)/1)+1 = 28\n",
        "        # Output Features = 8 x 28 x 28  \n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, \\\n",
        "                               stride=1, padding=1)\n",
        "        # Input Features = 8 x 28 x 28\n",
        "        # Output Features = 16 x 28 x 28 | ((28 - 3 + 2*1)/1)+1 = 28\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, \\\n",
        "                               stride=1, padding=1)\n",
        "\n",
        "        # Linear Layers \n",
        "        # What shape the first linear layer is going see?\n",
        "        # What are the total number of features given out by conv2?\n",
        "        # Features = 16 x 28 x 28 = 12544\n",
        "        # Therefore,           \n",
        "        self.linear1 = nn.Linear(in_features=12544, out_features=256)\n",
        "        self.linear2 = nn.Linear(in_features=256, out_features=64)\n",
        "        # Last linear layer should output 10 features as we are \n",
        "        # Classifying the images in 10 categories \n",
        "        self.linear3 = nn.Linear(in_features=64, out_features=10)\n",
        "\n",
        "        # Dropout \n",
        "        self.dropout = nn.Dropout(p=0.25)\n",
        "\n",
        "    # Here we define the 'forward behaviour' of our neural architecture \n",
        "    def forward(self, image_batch):\n",
        "        # This is also the place where we add ACTIVATION functions \n",
        "        image_batch = F.relu(input=self.conv1(image_batch))          \n",
        "        image_batch = F.relu(input=self.conv2(image_batch))  \n",
        "        \n",
        "        # Remember that when passing image_batch through the Linear layers, \n",
        "        # PyTorch expects: \n",
        "        # >>> torch.Size([12, 256]) -> example values \n",
        "            # 2d: [batch_size, num_features (aka: C * H * W)]\n",
        "            # use for nn.Linear() input.   \n",
        "        # Therefore, we need to 'flatten' image_batch\n",
        "        # image_batch = image_batch.view(batch_size, -1) --> batch size ???\n",
        "        flat_image_batch = image_batch.view(image_batch.shape[0], -1)\n",
        "        flat_image_batch = F.relu(input=self.linear1(flat_image_batch))\n",
        "        # Let's add the dropout too \n",
        "        flat_image_batch = self.dropout(F.relu(input=self.linear2(flat_image_batch)))\n",
        "        # Final Layer of the network \n",
        "        flat_image_batch = F.relu(input=self.linear3(flat_image_batch))\n",
        "        # The output from the final layer is a tensor with 10 'logits'\n",
        "        return flat_image_batch               "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEPNIgRiS9FN",
        "colab_type": "text"
      },
      "source": [
        "Now that we have defined our model, is there a way we can peep inside to see  \n",
        "what is going on and that if everything is alright?  \n",
        "\n",
        "Guess it's time to install _**[torchsummary !!!](https://github.com/sksq96/pytorch-summary)**_ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHTtCrNxQ_X7",
        "colab_type": "code",
        "outputId": "85ffe302-6463-46c3-f69d-9e3158a60108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Let's install torchsummary and do some cool stuff \n",
        "!pip install torchsummary # https://github.com/sksq96/pytorch-summary "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NylWMGTf1QbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's import some cool functions from these libraries \n",
        "# from poutyne.framework import Model # The core datastructure of poutyne \n",
        "# from poutyne.framework import ModelCheckpoint # Saves trained model during training\n",
        "# from poutyne.framework import Callback # We will see it in action in a little while \n",
        "\n",
        "from torchsummary import summary "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtUFlNAd5lhN",
        "colab_type": "code",
        "outputId": "4b8672d2-5a26-4cc2-9ce8-d8962bd7b6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# We can make the use of torchsummary library here to figure \n",
        "# if we have done something wrong \n",
        "\n",
        "# But first we need to tell PyTorch where to 'keep' the model \n",
        "# On GPU or on CPU \n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "print('The model will run on', device)\n",
        "\n",
        "# Initialize the model \n",
        "mnist1 = MNISTModel1().to(device)\n",
        "summary(model=mnist1, input_size=(1, 28, 28), batch_size=20) # Summarize"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model will run on cpu\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [20, 8, 28, 28]              80\n",
            "            Conv2d-2           [20, 16, 28, 28]           1,168\n",
            "            Linear-3                  [20, 256]       3,211,520\n",
            "            Linear-4                   [20, 64]          16,448\n",
            "           Dropout-5                   [20, 64]               0\n",
            "            Linear-6                   [20, 10]             650\n",
            "================================================================\n",
            "Total params: 3,229,866\n",
            "Trainable params: 3,229,866\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.06\n",
            "Forward/backward pass size (MB): 2.93\n",
            "Params size (MB): 12.32\n",
            "Estimated Total Size (MB): 15.31\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyTvkdq3gqHM",
        "colab_type": "text"
      },
      "source": [
        "That was a lot of work.....Whew!  \n",
        "\n",
        "_***Q: Isn't there an 'easier' way to define the Model in 2020?***_  \n",
        "A: _**Yes, absolutely!**_  \n",
        "Say hello to the _**[torchlayers library !!!](https://github.com/szymonmaszke/torchlayers)**_  \n",
        "With torchlayers, the above code will be reduced to about 7-8 lines!  \n",
        "But unfortunately, [torchlayers requires Python 3.7](https://github.com/szymonmaszke/torchlayers/issues/5) and above. Colab only  \n",
        "supports Python 3.6.x. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YljS2tiRlHql",
        "colab_type": "text"
      },
      "source": [
        "With our model definition complete, it is time to train! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f08VsPbPn5bO",
        "colab_type": "text"
      },
      "source": [
        "### 5. Train a PyTorch Model in 2020  \n",
        "\n",
        "PyTorch is infamous among newcomers for it's _'training loops'_.  \n",
        "They can be long and at times a little confusing too. However, most of them   \n",
        "are similar and writing training loops simply turns out to be a boring and repetitive  \n",
        "exercise.  \n",
        "\n",
        "Q: _**This is 2020.**_ Is there a better way?  \n",
        "A: _**Yes, absolutely!**_  \n",
        "Say hello to _**[Poutyne !!!](https://poutyne.org/index.html)**_   \n",
        "\n",
        "\n",
        "Thanks to _Poutyne_, writing training loops in PyTorch is _**FUN !!!**_  \n",
        "\n",
        "PS - Poutyne is pronounced as Poutine or Pu-tin.  \n",
        "![Poutyne](https://drive.google.com/uc?id=142xYy_mJoPSk97SDicvn9zRNxHMpXDxz \"You think loops are boring?!?!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gpy_1cRZ6gV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "aec78bef-3a5e-4b39-b159-549fd095a71b"
      },
      "source": [
        "# Install Poutyne \n",
        "!pip install poutyne "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: poutyne in /usr/local/lib/python3.6/dist-packages (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.18.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from poutyne) (1.4.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnHdjqkhdzLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import some useful functions from the Poutybne library \n",
        "from poutyne.framework import Model # The core datastructure of poutyne \n",
        "                                    # https://poutyne.org/model.html\n",
        "from poutyne.framework import ModelCheckpoint # Saves trained model during training\n",
        "                                              # https://poutyne.org/callbacks.html#checkpointing\n",
        "from poutyne.framework import Callback # We will see it in action in a little while \n",
        "                                       # https://poutyne.org/callbacks.html# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHVhp0yEiFfs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim # Optimizer: we need it to train our network"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EwMBpBtKE7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A pouytne training loop\n",
        "\n",
        "# A few hyperparamters for the training loop \n",
        "learning_rate = 0.1\n",
        "epochs = 1\n",
        "\n",
        "def poutyne_train(pytorch_network):\n",
        "    \n",
        "    # Select the optimizer and the loss function \n",
        "    optimizer = optim.SGD(pytorch_network.parameters(), lr=learning_rate)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    # Poutyne Model\n",
        "    model = Model(pytorch_network, optimizer, loss_function, batch_metrics=['accuracy'])\n",
        "    # Send the 'Poutyne model' on GPU/CPU whichever is available \n",
        "    model.to(device)\n",
        "    # Train\n",
        "    model.fit_generator(train_loader, valid_loader, epochs=epochs)\n",
        "    # Test\n",
        "    test_loss, test_acc = model.evaluate_generator(test_loader)\n",
        "    print(f'Test:\\n\\tLoss: {test_loss: .3f}\\n\\tAccuracy: {test_acc: .3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_znXT5djwzeY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "47ded4eb-8b29-434b-ceab-7561291a7268"
      },
      "source": [
        "# Let's start the training guys!!! \n",
        "poutyne_train(mnist1)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1 83.49s Step 2400/2400: loss: 0.023527, acc: 99.262500, val_loss: 0.070297, val_acc: 98.225000\n",
            "Test:\n",
            "\tLoss:  0.059\n",
            "\tAccuracy:  98.510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqforBpLr0dk",
        "colab_type": "text"
      },
      "source": [
        "_**CONGRATULATIONS !!! You just trained your first(?) CNN Model!**_   \n",
        "The accuracy looks pretty decent as well! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapbMW_qtfZZ",
        "colab_type": "text"
      },
      "source": [
        "### 6. Understanding Overfitting \n",
        "\n",
        "However, we are not done yet.  \n",
        "When we are training large deep learning models on massive datasets, the  \n",
        "training can take days and sometimes, [even weeks!](https://ai.googleblog.com/2016/03/train-your-own-image-classifier-with.html \"2 Weeks to train Inception v3!\")  \n",
        "\n",
        "When training on such large datasets, we need to make sure that we are saving   \n",
        "our best model as and when it gets trained after every epoch, lest something   \n",
        "gets awry and we lose days/weeks worth of training (and a lot of money  \n",
        "spent on the electricity!). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hakTX49k5wnW",
        "colab_type": "text"
      },
      "source": [
        "Before we move any further we need to understand the concept of _**Overfitting**_  \n",
        "in Machine Learning models.  \n",
        "What is _**overfitting?**_  \n",
        "![Fitting Examples](https://drive.google.com/uc?id=1gFOa5I24S7XDDep4WaVMIoivh9JvIp1Y \"https://www.curiousily.com/posts/hackers-guide-to-fixing-underfitting-and-overfitting-models/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnE0htQzVRAs",
        "colab_type": "text"
      },
      "source": [
        "We are always looking to ensure that our models have a low bias with a low  \n",
        "variance.  \n",
        "However, if we train for too long (too many epochs) our model will start to  \n",
        "overfit.  \n",
        "\n",
        "How do we identify that the model has started overfitting?  \n",
        "![Overfitting](https://drive.google.com/uc?id=1q02q0ge0jldHJm8P_Hq6ECeIvaSlcc36 \"https://mlexplained.com/2018/04/24/overfitting-isnt-simple-overfitting-re-explained-with-priors-biases-and-no-free-lunch/\")  \n",
        "Dotted vertical line is where we should either stop training the model (also  \n",
        "known as _**Early Stopping**_) or we should have some logic in the training  \n",
        "loop that _'saves'_ the model around that time while the training still  \n",
        "continues. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6qwm4NIZQNf",
        "colab_type": "text"
      },
      "source": [
        "Our model will also overfit if it is too complex.  \n",
        "For example (y-axis is different),  \n",
        "![Complex Model](https://drive.google.com/uc?id=1vWHgknPrbXEQczdKMliB0RyVgYld3cGa \"https://medium.com/@george.drakos62/cross-validation-70289113a072\")  \n",
        "\n",
        "<br/>Early stopping can save the day!  \n",
        "![alt text](https://drive.google.com/uc?id=1HnBFMWZGHy0UFMUICKU25qBiB-4_G0Xn \"https://www.jeremyjordan.me/deep-neural-networks-preventing-overfitting/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhvAC7xGapnU",
        "colab_type": "text"
      },
      "source": [
        "A really nice matrix to identify if we are overfitting:  \n",
        "![Overfitting Matrix](https://drive.google.com/uc?id=19bFHepjNgQ9kpqmQdMEW4lDJ-qXi5WF8 \"https://hackernoon.com/memorizing-is-not-learning-6-tricks-to-prevent-overfitting-in-machine-learning-820b091dc42\")  \n",
        "While preparing the above matrix, the author has considered only two sets,  \n",
        "training and testing (kinda _faux pass_!). Since we have a validation set as  \n",
        "well, replace the word _**'Testing'**_ with _**'Validation'**_. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qlOIi5RaeVI",
        "colab_type": "text"
      },
      "source": [
        "How can we avoid _**Overfitting**_?  \n",
        "1. Use a simpler model (less layers)  \n",
        "2. Use _Dropout_ \n",
        "3. Get more training data (if possible) \n",
        "4. Augment the data and add noise  \n",
        "5. Early Stopping  \n",
        "\n",
        "PS - Not an exhaustive list AT ALL.  \n",
        "\n",
        "<br/>Can we write a better training loop now? "
      ]
    }
  ]
}