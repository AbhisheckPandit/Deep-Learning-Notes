{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kevin's_Q&A_Session_Notes_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMtXiNl0+vGM/yf/DplIeZ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranjalchaubey/Deep-Learning-Notes/blob/master/Kevin's%20Data%20Science%20Class%20Notes/Kevin's_Q%26A_Session_Notes_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3hiIDhbDvmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd \n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder \n",
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.pipeline import make_pipeline "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cckw3TYXS-5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cols = ['Parch', 'Fare', 'Embarked', 'Sex', 'Name', 'Age']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRIPyMoATO3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('http://bit.ly/kaggletrain')\n",
        "X = df[cols]\n",
        "y = df['Survived']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2JVVb0ATZpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_new = pd.read_csv('http://bit.ly/kaggletest')\n",
        "X_new = df_new[cols]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8H4oYDVtTj16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp_constant = SimpleImputer(strategy='constant', fill_value='missing')\n",
        "ohe = OneHotEncoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAKhXCBuT2az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imp_ohe = make_pipeline(imp_constant, ohe)\n",
        "vect = CountVectorizer()\n",
        "imp = SimpleImputer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OzQTLi-UBed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = make_column_transformer(\n",
        "    (imp_ohe, ['Embarked', 'Sex']),\n",
        "    (vect, 'Name'),\n",
        "    (imp, ['Age', 'Fare']),\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IWrRhLAUUxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logreg = LogisticRegression(solver='liblinear', random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxVyLIP1ZL03",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "f5fd23d9-4669-48d7-e573-32d247797a25"
      },
      "source": [
        "pipe = make_pipeline(ct, logreg)\n",
        "pipe.fit(X, y)\n",
        "pipe.predict(X_new)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1,\n",
              "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
              "       1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n",
              "       1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
              "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
              "       1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
              "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
              "       1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
              "       1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
              "       1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
              "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
              "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsoSGudXZVRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A column transformer operates in PARALLEL\n",
        "# A pipeline operates SEQUENTIALLY\n",
        "\n",
        "# Therefore, different steps in a column transformer \n",
        "# are executed IN PARALELL, and NOT SEQUENTIALLY. \n",
        "\n",
        "# Output columns from the various 'steps' of a \n",
        "# Column Transformer are STACKED SIDE BY SIDE! "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y6gT7P4cf4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct_no_name = make_column_transformer(\n",
        "    (imp_ohe, ['Embarked', 'Sex']),\n",
        "    ('drop', 'Name'),   # Neat little trick to drop a Column from the transformer\n",
        "                        # Great for quick experimentation    \n",
        "    (imp, ['Age', 'Fare']),\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV1aBHX2hwr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There is no way to know how accurate your model COULD GET on a given dataset. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgLMYGKh32e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cross_val_score() under the hood: \n",
        "# for each fold in the dataset:\n",
        "#   1. a new 20% is set aside for testing, rest 80% for training --> SPLIT\n",
        "#   2. pipe.fit() runs of the 80% and learns                     --> TRANSFORM + FIT\n",
        "#   3. pipe.predict() runs on the 20% testing data               --> PREDICTION\n",
        "#   4. Prediction Accuracy calculated on the 20% test data       --> ACCURACY\n",
        "\n",
        "# For each SPLIT step, the imputation values are calculated separately for each fold. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b03K5CRtkVfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pipelines save you from Data Leakage. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvUHzRz_lAAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stratified - Sampling in such a way so that 'class frequencies' are the same \n",
        "#              in EACH SPLIT. \n",
        "\n",
        "# By default, StratifiedKFold DOES NOT shuffle rows. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhS2ZMM4mrOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.pipeline import make_union\n",
        "\n",
        "# make_union - Feature Union - Stacks the features side-by-side\n",
        "#            - Applies MULTIPLE TRANSFORMATIONS to a SINGLE COLUMN and \n",
        "#              stacks the results side by side\n",
        "\n",
        "# column_transformer - Applies a DIFFERENT TRANSFORMATION to EACH input column\n",
        "#                      and stacks the results side-by-side. \n",
        "\n",
        "# A Feature Union works on ONLY ONE COLUMN. \n",
        "# A column transformer works on an ARBITRARY NUMBER OF COLUMNS.\n",
        "# Both apply the transformations in PARALLEL!!!\n",
        "\n",
        "# FU are not much useful in front of column transformers. \n",
        "# So a BIG FU to FUs. Sorry. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bttVDkn2qps7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer \n",
        "\n",
        "# Iterative Imputer - runs a regression model and uses it to predict the missing values. \n",
        "#                     Needs other cols apart from the target column, as features \n",
        "#                     for the regression model. \n",
        "#                     Supplied columns CAN HAVE missing values. It will run as \n",
        "#                     many regression models as the cols that have missing values. \n",
        "#                     You can also specify the regression model being used. \n",
        "\n",
        "from sklearn.impute import KNNImputer # NOT experimental \n",
        "\n",
        "# KNNImputer - Utilizes the k-Nearest Neighbors to find the missing values. \n",
        "\n",
        "# In most cases, Iterative and KNN imputers will produce better results than the \n",
        "# SimpleImputer. \n",
        "\n",
        "# ONLY numerical columns work with Iterative and KNN Imputer. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7GNciidq6D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can automate the process of feature selection through pipelines. \n",
        "from sklearn.feature_selection import SelectPercentile, chi2\n",
        "\n",
        "# SelectPercentile - Uses statistical tests (chi2 in this case) to score the \n",
        "#                    the features.\n",
        "\n",
        "selection = SelectPercentile(chi2, percentile=50) # Keep top 50% of the features\n",
        "pipe_selection = make_pipeline(ct, selection, logreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F07KR8ktVDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can also select the features based of _coef importance of the models \n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "logreg_selection = LogisticRegression(solver='liblinear', penalty='l1', random_state=1)\n",
        "selection = SelectFromModel(logreg_selection, threshold='mean')\n",
        "pipe_selection = make_pipeline(ct, selection, logreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgOUGUF5tbTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some models benefit from Feature Standardization \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "imp_scaler = make_pipeline(imp, scaler)\n",
        "\n",
        "ct_scaler = make_column_transformer(\n",
        "    (imp_ohe, ['Embarked', 'Sex']),\n",
        "    (vect, 'Name'),\n",
        "    (imp_scaler, ['Age', 'Fare']),\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "pipe_scaler = make_pipeline(ct_scaler, logreg)\n",
        "\n",
        "# You do not always have to do the Standardization\n",
        "# Our solver, liblinear, is pretty robust. \n",
        "# liblinear doesn't require standardized inputs MOST OF THE TIMES. \n",
        "\n",
        "# DO NOT use the StandardScaler for a sparse matrix. \n",
        "# StandardScaler DESTROYS the sparsity of the matrix. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3mfnMih3kKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# An alternative way to scale, is to scale ALL THE COLUMNS. \n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "scaler = MaxAbsScaler() # PRESERVES SPARSITY\n",
        "                        # Scales everything coming out of the column transformer\n",
        "pipe_scaler = make_pipeline(ct, scaler, logreg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQD_iKTi5OgK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To Handle Outliers \n",
        "# 1. Use ROBUST SCALER \n",
        "# 2. Identify and remove them \n",
        "\n",
        "# column transformers DO NOT REMOVE ROWS. \n",
        "# That's what they are called column transformers. \n",
        "\n",
        "# You CAN USE pandas to remove outliers. No issue of data leakage here. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqSbn3NJ6FsJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom Transformation Functions \n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "import numpy as np \n",
        "\n",
        "get_floor  = FunctionTransformer(np.floor) # Custom Transformation "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6-EYBnW7-Fn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You should alwyas try to ensure that your custom tranformations \n",
        "# work well with both, pandas dataframes and the numpy arrays. \n",
        "def first_letter(df):\n",
        "    return pd.DataFrame(df).apply(lambda x: x.str.slice(0,1))\n",
        "\n",
        "get_first_letter = FunctionTransformer(first_letter)\n",
        "\n",
        "# Shape considerations for custom transformer functions \n",
        "# 1. They should ideally accept 2D inputs --> can pass multiple columns \n",
        "# 2. column transformer REQUIRES that all the functions output a 2D object"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nILTRSbKEHIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# If there are some rare categories in transformed dataset, that may \n",
        "# cause a problem during the cross validation. \n",
        "# It might happen that all of them might end up in the testing fold.\n",
        "# We need to be prepared for such a case. \n",
        "ohe_ignore = OneHotEncoder(handle_unknown='ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}